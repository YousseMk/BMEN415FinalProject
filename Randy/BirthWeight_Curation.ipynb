{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"/Users/RJ/Downloads/data/2008_births.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#birthweight is what we want to predict\n",
    "birth_weight = data[['BPOUND', 'BOUNCE']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA might be a good technique to select predictors \n",
    "\n",
    "#note that PCA performs best when data is normalized (range b/w 0 and 1)\n",
    "\n",
    "#It is possible to use categorical and continuous predictors \n",
    "#for a regression problem. My understanding is you need to make \n",
    "#dummy variables for the binary predictors. \n",
    "\n",
    "#Variables that we will need to deal with: \n",
    "# BDATE, HISPMOM, HISPDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting PCA on data\n",
    "#for now I drop the BDATE, HISPMOM AND HISPDAD\n",
    "data_drop = data.drop([\"BDATE\", \"HISPMOM\", \"HISPDAD\", \"BOUNCE\", \"BPOUND\"], axis = 1) #axis = 1 means to drop column not row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of columns in pandas object \n",
    "names_of_data = data_drop.columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_drop, birth_weight) \n",
    "\n",
    "\n",
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#running the actual PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "[5.14424319e-01 3.71701564e-02 3.01571080e-02 3.00306860e-02\n",
      " 2.70246380e-02 2.16962206e-02 1.91054659e-02 1.80162886e-02\n",
      " 1.70923972e-02 1.68531465e-02 1.53717986e-02 1.48402683e-02\n",
      " 1.40483819e-02 1.25382694e-02 1.13194236e-02 9.59814182e-03\n",
      " 9.10161124e-03 8.95652675e-03 8.70993926e-03 8.47433161e-03\n",
      " 8.47030448e-03 8.37926638e-03 7.98627690e-03 7.57029647e-03\n",
      " 7.34613334e-03 6.95961604e-03 6.70238715e-03 6.14390599e-03\n",
      " 6.06285273e-03 5.86525000e-03 5.29955490e-03 4.98235123e-03\n",
      " 4.82585062e-03 4.44349664e-03 4.30110987e-03 3.79385711e-03\n",
      " 3.77735304e-03 3.58115264e-03 3.55658465e-03 3.19045697e-03\n",
      " 3.05117741e-03 2.98289769e-03 2.70720335e-03 2.48879976e-03\n",
      " 2.21944943e-03 1.83236433e-03 1.64009918e-03 1.58043982e-03\n",
      " 1.45629207e-03 1.42018909e-03 1.39415111e-03 1.33995687e-03\n",
      " 1.27738982e-03 1.22230779e-03 1.12646024e-03 1.06422704e-03\n",
      " 9.66440491e-04 9.16980084e-04 8.63696128e-04 8.23939347e-04\n",
      " 7.84490693e-04 6.44588955e-04 6.20539238e-04 6.09979122e-04\n",
      " 5.87983358e-04 5.58612813e-04 5.56392509e-04 4.91665725e-04\n",
      " 4.46890630e-04 3.80569869e-04 3.71848298e-04 3.22204902e-04\n",
      " 3.14786752e-04 2.83411764e-04 2.71777128e-04 2.30253798e-04\n",
      " 2.15734053e-04 1.86546881e-04 1.85472937e-04 1.72253691e-04\n",
      " 1.68827495e-04 1.62564953e-04 1.44239367e-04 1.22548993e-04\n",
      " 1.20782658e-04 1.10578096e-04 1.05400003e-04 1.01082932e-04\n",
      " 7.96268159e-05 6.75354419e-05 5.88386083e-05 4.52495301e-05\n",
      " 3.97965345e-05 3.67301135e-05 3.05391955e-05 2.73842088e-05\n",
      " 2.35254371e-05 2.29435676e-05 2.19658947e-05 2.04491812e-05\n",
      " 1.90620235e-05 1.75051198e-05 1.12373042e-05 9.73389959e-06\n",
      " 9.08232353e-06 8.05967657e-06 7.14852616e-06 5.81928188e-06\n",
      " 5.57943333e-06 4.99909038e-06 4.81546660e-06 3.86015445e-06\n",
      " 2.76640162e-06 2.04392969e-06 3.87743555e-08 1.01452308e-31\n",
      " 2.49309176e-33 2.49309176e-33 2.49309176e-33 2.49309176e-33]\n"
     ]
    }
   ],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(len(explained_variance))\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance prints the variance each principal component contributes.\n",
    "#As we can see, the last 5 contribute very little (maybe we can get rid of?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
